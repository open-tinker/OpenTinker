# Configuration for Generic Environment Training (LLM-Environment Interaction)
#
# Key differences from opentinker_param.yaml:
# - algorithm: "agent_loop" (uses GenericAgentLoop)
# - No reward function (environment provides rewards)
# - Interaction configuration for Gym environment

# Server settings
scheduler_url: "http://localhost:8766"
scheduler_api_key: "otk_98b8db24ccd64c92e1fdd9a232e209fa"
env_url: "http://localhost:8084"  # Environment server URL

# GPU allocation
num_gpus: 4

# Data configuration
data_path: null  # Path to training data (parquet file)
val_data_path: null  # Optional validation data
tokenizer_path: null  # Path to tokenizer/model
batch_size: 16  # Smaller batch for multi-turn
val_batch_size: 50  # Validation batch size (also controls dataset size if val_max_samples not set)
# Training duration - set ONE of these (num_steps takes precedence if both set)
num_epochs: 3     # Number of epochs (null = use num_steps)
num_steps: null   # Total training steps (null = use num_epochs)
num_workers: 0

# Interaction configuration (for GenericAgentLoop)
# This replaces the reward function - environment provides rewards
interaction:
    name: "gym_env"  # Name referenced in dataset's interaction_kwargs
    class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
    config:
        env_endpoint: "http://localhost:8084"  # Mock environment server
        max_steps: 50
        observation_template: "Environment: {observation}"

# Multi-turn configuration
multi_turn:
    max_user_turns: 12
    max_assistant_turns: 12
    max_tokens_per_turn: 512  # Per-turn response limit (optional, null for no limit)
    # Weave tracing (optional - runs on SERVER side)
    weave_project: "zsqzz/generic-env-test"  # W&B project for tracing {'message': 'Entity opentinker not found', 'path': ['upsertModel']}
    experiment_name: "gym_interaction"  # Experiment name in Weave

# Project tracking
project_name: "generic_env_training"
experiment_name: "gym_interaction"
save_freq: 50000
test_freq: 5

# Generation parameters
temperature: 1  # Lower temperature for more focused responses
top_p: 1
max_new_tokens: 8192  # TOTAL response budget for entire multi-turn trajectory (NOT per-turn!)
max_prompt_tokens: 2048

# Algorithm selection
# IMPORTANT: Must be "agent_loop" for GenericEnvironment
algorithm: "agent_loop"

# Logging
logger_backends: ["console","wandb"]  # Add "wandb" if needed
wandb_key: null
enable_tracing: true
# No reward configuration needed!
# GenericEnvironment gets rewards from the interaction
