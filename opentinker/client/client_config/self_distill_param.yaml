# On-Policy Self-Distillation (OPSD) Configuration
# Paper: arXiv:2601.18734 - Self-Distilled Reasoner
#
# Teacher: p_T(.|x, y*) = p_θ0(.|x, y*) — FROZEN initial model conditioned on solution
# Student: p_S(.|x) = p_θ(.|x) — current model observes only the problem
#
# The teacher is the base model (without LoRA adapters), not the updating model.
#
# Usage:
#   python self_distill_rl.py \
#     tokenizer_path=Qwen/Qwen2.5-7B-Instruct \
#     data_path=/path/to/math_train.parquet

# Project settings
project_name: opentinker
experiment_name: self_distillation_opsd

# Logging
logger_backends: ["console", "wandb"]

# WandB (optional)
wandb_key: null

# Model (single model serves as both teacher and student)
tokenizer_path: Qwen/Qwen2.5-7B-Instruct

# Data paths
data_path: null  # Path to training data (parquet/jsonl) with prompt + ground_truth
val_data_path: null

# Training parameters
batch_size: 32
num_workers: 0
num_epochs: 4
num_steps: null
save_freq: 100
test_freq: 5

# Validation parameters
val_batch_size: 32
# System prompt for validation data. null = same as training (\boxed{} format).
val_system_prompt: null
# data_source for validation reward routing. Must match a key in verl's default_compute_score.
# "DigitalLearningGmbH/MATH-lighteval" → math_reward (extracts \boxed{})
# "math" → math_dapo (extracts Answer:)
# null = use interaction_name ("math")
val_data_source: "DigitalLearningGmbH/MATH-lighteval"
# Column names for validation data (AIME: Problem/Answer, MATH: prompt/ground_truth)
val_prompt_key: "Problem"
val_ground_truth_key: "Answer"

# Generation parameters (for student rollouts)
temperature: 1.2          # Paper uses 1.2
top_p: 1
max_new_tokens: 8192      # Paper uses 2048 (4-8x more efficient than GRPO)
max_prompt_tokens: 2048

# Algorithm (GRPO structure for batch compatibility)
algorithm: "single_turn"
adv_estimator: "grpo"     # Required for batch structure, not used in self-distillation
rollout_n: 1              # Paper: 1 sample per prompt (vs GRPO's 8)

# Self-distillation settings (OPSD)
self_distillation:
  loss_type: "sampled_token"      # "sampled_token" (Eq.9 in paper, recommended) or "jsd" (per-token approx)
  beta: 0.5                       # JSD interpolation: 0.5 = symmetric JSD
  clip_advantage: 0.0             # Clip advantages for sampled_token loss (0 = no clipping)
  solution_key: "Solution"    # Dataset field containing privileged solution text
  solution_template: "\n\nReference solution: {solution}\n\n After understanding the reference solution, please try to solve this problem using your own approach below:"
  max_solution_length: 512        # Max tokens for solution text

# LoRA configuration (paper uses LoRA rank 64)
lora:
  lora_rank: 64
  lora_alpha: 128
  target_modules: "all-linear"
  exclude_modules: null
  lora_adapter_path: null

# Interaction configuration
interaction:
  name: self_distill
  class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
  config:
    env_host: 0.0.0.0
    env_port: 8088
    env_endpoint: http://${interaction.config.env_host}:${interaction.config.env_port}
    max_steps: 1

multi_turn:
  max_user_turns: 0
  max_assistant_turns: 1
  max_tokens_per_turn: 2048
  weave_project: null
  experiment_name: "self_distill_interaction"

# Scheduler settings
scheduler_url: "http://0.0.0.0:8780"
scheduler_api_key: otk_98b8db24ccd64c92e1fdd9a232e209fa

# GPU settings
num_gpus: 4
