# Math Training Configuration (GameEnvironment Pattern)
# Use with: python math_client_unified.py

# Project settings
project_name: opentinker
experiment_name: math_training

# Logging
logger_backends: ["console", "wandb"]

# Tracing (optional)
enable_tracing: true
weave_project: null

# WandB (optional)
wandb_key: null

# Model and tokenizer
tokenizer_path: null

# Data paths
data_path: null        # Path to training data (JSON/JSONL)
val_data_path: null    # Path to validation data (JSON/JSONL)

# Training parameters
batch_size: 64
num_workers: 0
# Training duration - set ONE of these (num_steps takes precedence if both set)
num_epochs: 10        # Number of epochs (null = use num_steps)
num_steps: null       # Total training steps (null = use num_epochs)
save_freq: 100
test_freq: 50        # Validation frequency (every N steps)

# Validation parameters
val_batch_size: 100   # Total validation samples

# Model parameters
# Generation parameters
temperature: 1
top_p: 1
max_new_tokens: 4098  # TOTAL response budget for entire trajectory
max_prompt_tokens: 4096

# Algorithm - toolcall for math with tool use
algorithm: "agent_loop"


# RL Algorithm settings (passed to server via scheduler)
# adv_estimator: "grpo" or "gae" (for PPO)
adv_estimator: "grpo"
# rollout_n: number of samples per prompt for GRPO (only used when adv_estimator=grpo)
rollout_n: 8


# Interaction configuration
interaction:
  name: math
  class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
  config:
    env_endpoint: ${env_url}  # References top-level env_url
    max_steps: 1     # Max interaction steps

multi_turn:
    max_user_turns: 0
    max_assistant_turns: 1
    max_tokens_per_turn: 4096  # Per-turn response limit
    weave_project: null
    experiment_name: "math_interaction"

# Server settings
scheduler_url: "http://0.0.0.0:8780"
scheduler_api_key: otk_98b8db24ccd64c92e1fdd9a232e209fa
env_url: "http://0.0.0.0:8088"  # Math environment server URL

# GPU settings
num_gpus: 4
