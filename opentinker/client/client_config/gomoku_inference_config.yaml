# Gomoku Inference Configuration
# Use with: python gomoku_inference.py

# Model settings
model_path: null           # Path to trained checkpoint (HuggingFace format)
tokenizer_path: null       # Tokenizer path (defaults to model_path if null)
vllm_server_url: null      # vLLM server URL for server mode (e.g., "http://localhost:8000")

# GPU settings (offline mode only)
tensor_parallel_size: 1    # Number of GPUs for tensor parallelism
gpu_memory_utilization: 0.9

# Generation parameters
temperature: 0.0           # 0.0 = greedy decoding
top_p: 1.0
max_new_tokens: 8192  # TOTAL response budget for entire multi-turn trajectory (NOT per-turn!)
max_prompt_tokens: 4096
max_context_length: 30000  # Max context before ending game (< model max 32768)

# Data settings (Gomoku uses dynamic generation, no data_path needed)
data_path: null            # Not needed for Gomoku (uses dynamic generation)
output_path: null          # Output results file (jsonl)
max_samples: 10            # Number of games to play

# Environment settings  
env_url: http://localhost:8091

# Multi-turn settings (Gomoku is multi-turn game)
multi_turn:
    max_user_turns: 39       # Max environment turns (moves)
    max_assistant_turns: 39  # Max model response turns
    max_tokens_per_turn: 256  # Per-turn response limit (optional, null for no limit)


# Game-specific settings
board_size: 9              # Gomoku board size (9x9)